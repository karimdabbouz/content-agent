Mathematically, the entropy H(X) of a discrete random variable X with possible values {x1, x2, ..., xn} and probability mass function P(X) is defined as H(X) = -Σ P(x) log₂ P(x), where the sum is over all possible values of X. This formula captures the expected value of the information contained in each possible outcome. If all outcomes are equally likely, entropy is maximized, indicating maximum uncertainty. Conversely, if one outcome is certain, entropy is zero, reflecting no uncertainty.